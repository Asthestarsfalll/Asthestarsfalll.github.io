---
title: Dynamic Convolution
authors:
  - Asthestarsfalll
tags:
  - PaperRead
  - attention
  - BaseOp
description: 引入动态卷积，即根据多个卷积核对每个输入的关注程度，动态地对它们进行聚合。与静态算法(每层只有一个卷积核)相比，该算法显著提高了网络的表示能力，但增加了大量的计算开销，对高效的网络神经网络更加友好。并且可以很容易地集成到现有的CNN架构中
hide_table_of_contents: false
---

> 论文名称：[Dynamic Convolution: Attention over Convolution Kernels](https://arxiv.org/pdf/1912.03458.pdf)
>
> 作者：Yinpeng Chen ，Xiyang Dai， Mengchen Liu， Dongdong Chen， Lu Yuan， Zicheng Liu
>
> Code：https://github.com/kaijieshi7/Dynamic-convolution-Pytorch（非官方）

## 摘要

- 相比高性能深度网络，轻量型网络因其低计算负载约束 (深度与通道方面的约束) 导致其存在性能降低，即比较有效的特征表达能力。为解决该问题，作者提出动态卷积：它可以提升模型表达能力而无需提升网络深度与宽度。

- 不同于常规卷积中的单一核，动态卷积根据输入动态的集成多个并行的卷积核为一个动态核，该动态核具有数据依赖性。多核集成不仅计算高效，而且具有更强的特征表达能力 (因为这些核通过注意力机制以非线性形式进行融合)。

- 通过简单地使用动态卷积的最新架构 MobilenetV3-Small，在 ImageNet 分类中排名前 1 的精度提高了 2.3%，仅增加了 4% 的错误，而在 COCO keypoint 检测中实现了 2.9 AP 增益。

  ![image-20210801165209269](/images/2022/03/27/image-20210801165209269.png)

## Dynamic Convolution

### 生成方法

![image-20210801165349986](/images/2022/03/27/image-20210801165349986.png)

动态卷积将 SE Block 生成的注意力对卷积核进行加权求和，最终得到一个随着输入变化的动态卷积核，其生成方法而下：

1. 使用 SE Block 生成注意力，需要注意的是，这里的输出通道数不再是原通道，而是 $K$，表示后面的 $conv_1\cdots conv_K$
2. 使用一个 $softmax$ 将注意力进行归一化​
3. 将得到的注意力与相应的静态卷积核进行广播相乘，加权求和得到最终的动态卷积

### 为什么要约束注意力输出

约束注意力输出可以促进注意力模型πk(x) 的学习。

具体来说，通过约束其和为 1，使得动态卷积核 $\tilde{W}=\sum_k{\pi_k\tilde{W}_k}$ 保持在核空间中 $\{\tilde{W}_k\}$​的凸包中，下面给出一个例子

![image-20210801171918465](/images/2022/03/27/image-20210801171918465.png)

在一个拥有三个卷积核的空间内，限制 $0<\pi_k(x)<1$​​只能将​动态核限制在这两个三角锥内，而限制 $\pi_k(x)=1$ 可以使动态核限制在中间的三角形区域，在上图中动态核被压缩在这一红点。显而易见，归一化使得 $\pi_k(x)$ 的学习更加简化。

### 训练早期的近似均匀注意力

在训练早期，近乎均匀的注意力可以促进所有内核 $\{\tilde{W_k}\}$​​ 的学习，这个观点在  中也有体现

作者在最初使用 $softmax$ 并没有获得很好的效果，甚至相较于原网络还有一定程度上的精度损失

这主要是因为 $softmax$ 的性质，详细可以见 [此](https://asthestarsfalll.icu/2021/06/04/transformer/#%E7%BC%A9%E6%94%BE%E7%82%B9%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9B)，对于方差较大的输入，其输出接近 $one-hot$​​编码，这使得注意力分布极其不均匀，因此使用了参数 ` temperature` 减小输入的方差，即类似与 transformer 中的缩放点积 ，从而获得了更加优秀的效果。

### 总结与思考

总的来说，本文为增加网络的复杂性提出了一种新的思路，不同与普通卷积，动态卷积会根据输入动态的调整卷积核的参数，而不是对所有输入一视同仁。不**一视同仁**是注意力机制方面都强调的东西，也是其所追求的目标。

本文同样也是一种动态核，需要注意的是，与 Transformer 和 Involution 等不同，他们的细粒度有所差异，本文虽然会对不同的输入动态调整核，但是对于输入的不同位置还是使用同一个核，而 Transformer 与 Involution 等对每一个位置都生成动态核。

**缺点**:

1. 虽然本文在增加极少运算量的情况下，显著的提高的轻量级网络的性能，对轻量级网络性能的提升有一定的帮助，虽然其运算量几乎没有变化，但是动态卷积网络的参数量是大幅增加的

2. 在训练时是一个二阶优化问题，相较于普通卷积更难以训练。
